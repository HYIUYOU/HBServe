# HBServe pdåˆ†ç¦»æ¶æ„

åŸºäºHBServeå®ç°çš„ç®€æ˜“pd(prefill-decode)åˆ†ç¦»æ¶æ„ï¼Œæ”¯æŒKVç¼“å­˜ä¼ è¾“ã€åˆ†å—prefillå’ŒCUDA graphä¼˜åŒ–ã€‚

## åŠŸèƒ½ç‰¹æ€§

### ğŸš€ æ ¸å¿ƒåŠŸèƒ½
- **pdåˆ†ç¦»æ¶æ„**: ç‹¬ç«‹çš„prefillå’Œdecodeå®ä¾‹
- **KVä¼ è¾“**: æ”¯æŒNCCLå’ŒMooncakeä¸¤ç§backend
- **RPCé€šä¿¡**: CPUè°ƒåº¦å™¨ä¸GPUå®ä¾‹é—´çš„é«˜æ•ˆé€šä¿¡
- **åˆ†å—Prefill**: æ”¯æŒé•¿åºåˆ—çš„åˆ†å—å¤„ç†
- **CUDA Graph**: è‡ªåŠ¨ä¼˜åŒ–é‡å¤è®¡ç®—æ¨¡å¼

### ğŸ“‹ æŠ€æœ¯ç‰¹ç‚¹
- **å¼‚æ­¥è°ƒåº¦**: CPUç«¯å¼‚æ­¥è°ƒåº¦ï¼Œæ”¯æŒé«˜å¹¶å‘
- **å†…å­˜ä¼˜åŒ–**: KVç¼“å­˜æŸ¥æ‰¾ç¼“å†²åŒºï¼Œå¤„ç†ä¹±åºè¯·æ±‚
- **å…¼å®¹æ¥å£**: ä¿æŒä¸åŸå§‹HBServeçš„æ¥å£å…¼å®¹
- **çµæ´»éƒ¨ç½²**: æ”¯æŒå•æœºå¤šGPUå’Œå¤šæœºéƒ¨ç½²

## æ¶æ„è®¾è®¡

```
[CPUè°ƒåº¦å™¨] --> RPC --> [Prefillå®ä¾‹(GPU-0)]
                              |
                           KVä¼ è¾“ (NCCL/Mooncake)
                              |
                              v
                        [Decodeå®ä¾‹(GPU-1)]
```

### ç»„ä»¶è¯´æ˜

1. **CPUè°ƒåº¦å™¨** (`CpuScheduler`)
   - ç®¡ç†è¯·æ±‚é˜Ÿåˆ—å’Œåˆ†å‘
   - åè°ƒprefillå’Œdecodeæµç¨‹
   - ç›‘æ§å®ä¾‹çŠ¶æ€

2. **GPUå®ä¾‹** (`GpuInstance`)
   - Prefillå®ä¾‹ï¼šå¤„ç†promptç¼–ç 
   - Decodeå®ä¾‹ï¼šå¤„ç†tokenç”Ÿæˆ
   - æ”¯æŒchunked prefillå’ŒCUDA graph

3. **KVä¼ è¾“** (`KVTransferManager`)
   - NCCL backendï¼šåŸºäºPyTorchåˆ†å¸ƒå¼
   - Mooncake backendï¼šé«˜æ€§èƒ½RDMAä¼ è¾“
   - æŸ¥æ‰¾ç¼“å†²åŒºå¤„ç†ä¹±åºè¯·æ±‚

4. **RPCé€šä¿¡** (`RpcClient/RpcServer`)
   - è½»é‡çº§TCPåè®®
   - å¼‚æ­¥éé˜»å¡é€šä¿¡
   - æ”¯æŒå¤šç§æ¶ˆæ¯ç±»å‹

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install torch transformers

# NCCLæ”¯æŒï¼ˆé€šå¸¸éšPyTorchå®‰è£…ï¼‰
# Mooncakeæ”¯æŒï¼ˆå¯é€‰ï¼‰
pip install mooncake  # æŒ‰ç…§Mooncakeæ–‡æ¡£å®‰è£…
```

### 2. ä¸‹è½½æ¨¡å‹

```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

### 3. è¿è¡Œç¤ºä¾‹

```python
import asyncio
from hbserve.pd_disagg import DisaggregatedLLM, PdDisaggConfig, KVTransferBackend
from hbserve.sampling_params import SamplingParams

# åˆ›å»ºé…ç½®
config = PdDisaggConfig(
    model_path="~/huggingface/Qwen3-0.6B/",
    kv_transfer_backend=KVTransferBackend.NCCL,
    chunked_prefill_enabled=True,
    cuda_graph_enabled=True
)

async def main():
    # åˆ›å»ºåˆ†ç¦»å¼LLM
    llm = DisaggregatedLLM(config)
    
    try:
        # å¯åŠ¨ç³»ç»Ÿ
        await llm.start()
        
        # ç”Ÿæˆæ–‡æœ¬
        prompts = ["ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½", "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"]
        sampling_params = SamplingParams(temperature=0.7, max_tokens=100)
        
        results = await llm.generate(prompts, sampling_params)
        
        for prompt, result in zip(prompts, results):
            print(f"Prompt: {prompt}")
            print(f"Response: {result['text']}")
            
    finally:
        await llm.stop()

asyncio.run(main())
```

### 4. å…¼å®¹æ¥å£

```python
# ä¸åŸå§‹HBServeå…¼å®¹çš„åŒæ­¥æ¥å£
from hbserve.pd_disagg import LLM
from hbserve.sampling_params import SamplingParams

llm = LLM("~/huggingface/Qwen3-0.6B/", 
          kv_transfer_backend="nccl",
          chunked_prefill_enabled=True)

sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, pd-disagg HBServe!"]
outputs = llm.generate(prompts, sampling_params)
print(outputs[0]["text"])
```

## é…ç½®è¯´æ˜

### åŸºç¡€é…ç½®

```python
config = PdDisaggConfig(
    # æ¨¡å‹é…ç½®
    model_path="~/huggingface/Qwen3-0.6B/",
    tensor_parallel_size=1,
    max_num_seqs=256,
    max_num_batched_tokens=2048,
    
    # KVä¼ è¾“é…ç½®
    kv_transfer_backend=KVTransferBackend.NCCL,  # æˆ– MOONCAKE
    kv_buffer_size=1024*1024*1024,  # 1GB
    
    # åˆ†å—prefillé…ç½®
    chunked_prefill_enabled=True,
    chunk_size=512,
    
    # CUDA graphé…ç½®
    cuda_graph_enabled=True,
    cuda_graph_max_seq_len=2048
)
```

### YAMLé…ç½®æ–‡ä»¶

```yaml
# configs/pd_disagg_config.yaml
model:
  path: "~/huggingface/Qwen3-0.6B/"
  tensor_parallel_size: 1

kv_transfer:
  backend: "nccl"  # "nccl" æˆ– "mooncake"
  buffer_size: 1073741824  # 1GB
  ip: "127.0.0.1"
  port: 12345

chunked_prefill:
  enabled: true
  chunk_size: 512

cuda_graph:
  enabled: true
  max_seq_len: 2048

gpu_instances:
  - instance_id: "prefill-0"
    instance_type: "prefill"
    gpu_id: 0
    host: "localhost"
    port: 50051
    kv_rank: 0
    
  - instance_id: "decode-0"
    instance_type: "decode"
    gpu_id: 1
    host: "localhost"
    port: 50052
    kv_rank: 1
```

## éƒ¨ç½²æ–¹å¼

### å•æœºéƒ¨ç½²

```bash
# å¯åŠ¨å®Œæ•´ç³»ç»Ÿ
python scripts/start_pd_disagg.py --config configs/pd_disagg_config.yaml

# æˆ–è€…åˆ†åˆ«å¯åŠ¨å®ä¾‹
python scripts/start_pd_disagg.py --config configs/pd_disagg_config.yaml --instance-id prefill-0
python scripts/start_pd_disagg.py --config configs/pd_disagg_config.yaml --instance-id decode-0
```

### å¤šæœºéƒ¨ç½²

```bash
# åœ¨prefillèŠ‚ç‚¹
python scripts/start_pd_disagg.py --config configs/pd_disagg_config.yaml --instance-id prefill-0

# åœ¨decodeèŠ‚ç‚¹  
python scripts/start_pd_disagg.py --config configs/pd_disagg_config.yaml --instance-id decode-0

# åœ¨è°ƒåº¦èŠ‚ç‚¹
python scripts/start_cpu_scheduler.py --config configs/pd_disagg_config.yaml
```

## KVä¼ è¾“Backend

### NCCL Backend

- **é€‚ç”¨åœºæ™¯**: å•æœºå¤šGPUæˆ–InfiniBandç½‘ç»œ
- **ä¼˜åŠ¿**: åŸç”ŸPyTorchæ”¯æŒï¼Œé…ç½®ç®€å•
- **é…ç½®**: è‡ªåŠ¨ä½¿ç”¨PyTorchåˆ†å¸ƒå¼

```python
config = PdDisaggConfig(
    kv_transfer_backend=KVTransferBackend.NCCL,
    kv_ip="127.0.0.1",
    kv_port=12345
)
```

### Mooncake Backend

- **é€‚ç”¨åœºæ™¯**: é«˜æ€§èƒ½RDMAç½‘ç»œ
- **ä¼˜åŠ¿**: æ›´ä½å»¶è¿Ÿï¼Œæ›´é«˜å¸¦å®½
- **ä¾èµ–**: éœ€è¦å®‰è£…Mooncakeåº“

```python
config = PdDisaggConfig(
    kv_transfer_backend=KVTransferBackend.MOONCAKE,
    kv_ip="10.0.0.1",
    kv_port=12346
)
```

```bash
# è®¾ç½®Mooncakeé…ç½®
export MOONCAKE_CONFIG_PATH=configs/mooncake_config.json
```

## åˆ†å—Prefill

### å·¥ä½œåŸç†

é•¿åºåˆ—è‡ªåŠ¨åˆ†å—å¤„ç†ï¼Œæ¯ä¸ªåˆ†å—ç‹¬ç«‹è¿›è¡Œprefillï¼Œæ”¯æŒï¼š

- **è‡ªé€‚åº”åˆ†å—**: æ ¹æ®åºåˆ—é•¿åº¦è‡ªåŠ¨åˆ†å—
- **CUDA Graphä¼˜åŒ–**: å›ºå®šå¤§å°åˆ†å—ä½¿ç”¨CUDA Graph
- **å†…å­˜ä¼˜åŒ–**: é¿å…é•¿åºåˆ—çš„å†…å­˜å³°å€¼

### é…ç½®å‚æ•°

```python
config = PdDisaggConfig(
    chunked_prefill_enabled=True,
    chunk_size=512,                    # åˆ†å—å¤§å°
    max_chunk_prefill_tokens=4096,     # æœ€å¤§åˆ†å—tokens
    cuda_graph_enabled=True            # å¯ç”¨CUDA Graph
)
```

## æ€§èƒ½ä¼˜åŒ–

### CUDA Graph

- **è‡ªåŠ¨ç¼“å­˜**: ç›¸åŒshapeçš„è®¡ç®—å›¾è‡ªåŠ¨ç¼“å­˜
- **é€‚ç”¨åœºæ™¯**: å›ºå®šbatch sizeå’Œåºåˆ—é•¿åº¦
- **æ€§èƒ½æå‡**: å‡å°‘GPU kernelå¯åŠ¨å¼€é”€

### å†…å­˜ä¼˜åŒ–

- **KVç¼“å­˜å¤ç”¨**: é«˜æ•ˆçš„KVç¼“å­˜ç®¡ç†
- **åˆ†å—å¤„ç†**: é¿å…é•¿åºåˆ—å†…å­˜æº¢å‡º
- **æŸ¥æ‰¾ç¼“å†²åŒº**: å¤„ç†ä¹±åºè¯·æ±‚ï¼Œæé«˜ååé‡

### æ‰¹å¤„ç†ä¼˜åŒ–

- **åŠ¨æ€æ‰¹å¤„ç†**: è‡ªåŠ¨ç»„åˆå°æ‰¹æ¬¡
- **å¼‚æ­¥è°ƒåº¦**: CPUå’ŒGPUå¹¶è¡Œå¤„ç†
- **è´Ÿè½½å‡è¡¡**: å¤šå®ä¾‹é—´è´Ÿè½½åˆ†é…

## ç›‘æ§å’Œè°ƒè¯•

### ç³»ç»Ÿç»Ÿè®¡

```python
# è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
stats = llm.get_system_stats()

print("CPUè°ƒåº¦å™¨ç»Ÿè®¡:")
print(f"  æ€»è¯·æ±‚æ•°: {stats['cpu_scheduler']['total_requests']}")
print(f"  å®Œæˆè¯·æ±‚æ•°: {stats['cpu_scheduler']['completed_requests']}")
print(f"  æˆåŠŸç‡: {stats['cpu_scheduler']['success_rate']:.2%}")

print("GPUå®ä¾‹ç»Ÿè®¡:")
for instance_id, instance_stats in stats["gpu_instances"].items():
    print(f"  {instance_id}:")
    print(f"    çŠ¶æ€: {instance_stats['status']}")
    print(f"    æ´»è·ƒè¯·æ±‚: {instance_stats['active_requests']}")
```

### æ—¥å¿—é…ç½®

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è°ƒè¯•æ—¥å¿—
python scripts/start_pd_disagg.py --config configs/pd_disagg_config.yaml --log-level DEBUG
```

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

1. **KVä¼ è¾“å¤±è´¥**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç«¯å£
   - éªŒè¯NCCL/Mooncakeå®‰è£…
   - ç¡®è®¤GPUè®¾å¤‡å¯è§æ€§

2. **RPCè¿æ¥è¶…æ—¶**
   - æ£€æŸ¥å®ä¾‹å¯åŠ¨é¡ºåº
   - éªŒè¯ç«¯å£é…ç½®
   - ç¡®è®¤é˜²ç«å¢™è®¾ç½®

3. **å†…å­˜ä¸è¶³**
   - å‡å°‘batch sizeæˆ–chunk size
   - å¯ç”¨åˆ†å—prefill
   - è°ƒæ•´KVç¼“å­˜å¤§å°

### æ€§èƒ½è°ƒä¼˜

1. **ä¼˜åŒ–åˆ†å—å¤§å°**
   ```python
   # æ ¹æ®GPUå†…å­˜è°ƒæ•´
   config.chunk_size = 256  # å‡å°‘å†…å­˜ä½¿ç”¨
   config.chunk_size = 1024 # æé«˜æ•ˆç‡
   ```

2. **è°ƒæ•´ç¼“å†²åŒºå¤§å°**
   ```python
   # å¹³è¡¡å†…å­˜å’Œæ€§èƒ½
   config.kv_buffer_size = 512 * 1024 * 1024  # 512MB
   ```

3. **CUDA Graphé…ç½®**
   ```python
   # é™åˆ¶CUDA Graphä½¿ç”¨èŒƒå›´
   config.cuda_graph_max_seq_len = 1024
   ```

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„KVä¼ è¾“Backend

```python
from hbserve.pd_disagg.kv_transfer.base import KVTransferBase

class CustomKVTransfer(KVTransferBase):
    async def send_kv_cache(self, request_id, kv_buffer, target_rank):
        # å®ç°è‡ªå®šä¹‰ä¼ è¾“é€»è¾‘
        pass
        
    async def receive_kv_cache(self, request_id, source_rank):
        # å®ç°è‡ªå®šä¹‰æ¥æ”¶é€»è¾‘
        pass
```

### è‡ªå®šä¹‰è°ƒåº¦ç­–ç•¥

```python
from hbserve.pd_disagg.cpu_scheduler import CpuScheduler

class CustomScheduler(CpuScheduler):
    async def _schedule_waiting_requests(self):
        # å®ç°è‡ªå®šä¹‰è°ƒåº¦é€»è¾‘
        pass
```

## è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## å‚è€ƒ

- [nanovllm](https://github.com/GeeeekExplorer/nanovllm)
- [vLLM](https://github.com/vllm-project/vllm)
- [Mooncake](https://github.com/kvcache-ai/Mooncake)
